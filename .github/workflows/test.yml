# .github/workflows/test.yml
name: Agent Tests
on: [push, pull_request]

permissions:
  contents: read
  pull-requests: write

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install dependencies
        run: uv sync

      # Make failures show as GitHub annotations
      - name: Install pytest annotations plugin
        run: uv pip install pytest-github-actions-annotate-failures

      - name: Run code formatting checks
        continue-on-error: true
        run: |
          echo "::notice::Running code formatting checks - issues will be shown as warnings"
          
          # Check black formatting
          if ! uv run black --check .; then
            echo "::warning::Black formatting issues found. Run 'uv run black .' to fix."
          fi
          
          # Check isort formatting
          if ! uv run isort --check-only .; then
            echo "::warning::Import sorting issues found. Run 'uv run isort .' to fix."
          fi

      - name: Run tests
        run: |
          mkdir -p reports
          uv run pytest tests/ -v \
            --cov=agent --cov-report=term-missing --cov-report=xml \
            --junitxml=reports/pytest-junit.xml
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          EXA_API_KEY: ${{ secrets.EXA_API_KEY }}

      - name: Publish test report
        uses: dorny/test-reporter@v2
        if: always()                      # run even if tests failed
        with:
          name: Pytest Results
          path: reports/pytest-junit.xml  # file produced by --junitxml
          reporter: java-junit            # works for pytestâ€™s JUnit XML
          use-actions-summary: true       # puts a table in the run Summary
          fail-on-error: true